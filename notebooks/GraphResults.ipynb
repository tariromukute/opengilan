{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "msg = \"Hello World\"\n",
                "print(msg)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Hello World\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "# configure spark variables\n",
                "from pyspark.context import SparkContext\n",
                "from pyspark.sql.context import SQLContext\n",
                "from pyspark.sql.session import SparkSession\n",
                "    \n",
                "sc = SparkContext()\n",
                "sqlContext = SQLContext(sc)\n",
                "spark = SparkSession(sc)\n",
                "\n",
                "# load up other dependencies\n",
                "import re\n",
                "import pandas as pd\n",
                "\n",
                "import glob\n",
                "\n",
                "raw_data_files = glob.glob('../ansible/results/bpftrace/server1-1200m-1.out')\n",
                "raw_data_files\n",
                "\n",
                "base_df = spark.read.text(raw_data_files)\n",
                "base_df.printSchema()\n",
                "\n",
                "base_df.show(10, truncate=False)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "21/09/09 19:55:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
                        "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
                        "Setting default log level to \"WARN\".\n",
                        "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "root\n",
                        " |-- value: string (nullable = true)\n",
                        "\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": []
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "+-------------------------------------------------------------+\n",
                        "|value                                                        |\n",
                        "+-------------------------------------------------------------+\n",
                        "|Attaching 12 probes...                                       |\n",
                        "|Tracing latency of network stack funtions. Hit Ctrl-C to end.|\n",
                        "|@q1[openssl]: count 140, average 1405, total 196805          |\n",
                        "|@q1[ksoftirqd/169]: count 3, average 3099, total 9297        |\n",
                        "|                                                             |\n",
                        "|@q2[ksoftirqd/169]: count 25, average 863, total 21583       |\n",
                        "|@q2[openssl]: count 870, average 6923, total 6023733         |\n",
                        "|                                                             |\n",
                        "|                                                             |\n",
                        "|@q4[openssl]: count 9809, average 955, total 9369210         |\n",
                        "+-------------------------------------------------------------+\n",
                        "only showing top 10 rows\n",
                        "\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": []
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "sample_logs = [item['value'] for item in base_df.take(15)]\n",
                "sample_logs"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "['Attaching 12 probes...',\n",
                            " 'Tracing latency of network stack funtions. Hit Ctrl-C to end.',\n",
                            " '@q1[openssl]: count 140, average 1405, total 196805',\n",
                            " '@q1[ksoftirqd/169]: count 3, average 3099, total 9297',\n",
                            " '',\n",
                            " '@q2[ksoftirqd/169]: count 25, average 863, total 21583',\n",
                            " '@q2[openssl]: count 870, average 6923, total 6023733',\n",
                            " '',\n",
                            " '',\n",
                            " '@q4[openssl]: count 9809, average 955, total 9369210',\n",
                            " '@q4[ksoftirqd/169]: count 115, average 969, total 111548',\n",
                            " '',\n",
                            " '@q5[openssl]: count 74695, average 432, total 32294636',\n",
                            " '@q5[ksoftirqd/169]: count 811, average 441, total 357987',\n",
                            " '']"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 3
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "bpftrace_ps_pattern = r'^@(q\\d)\\[([\\d\\D]*)\\]\\D*(\\d+)\\D*(\\d+)\\D*(\\d+)([\\d\\D]*)'\n",
                "ps = [re.search(bpftrace_ps_pattern, item).groups()\n",
                "           if re.search(bpftrace_ps_pattern, item)\n",
                "           else None\n",
                "           for item in sample_logs]\n",
                "ps"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "[None,\n",
                            " None,\n",
                            " ('q1', 'openssl', '140', '1405', '196805', ''),\n",
                            " ('q1', 'ksoftirqd/169', '3', '3099', '9297', ''),\n",
                            " None,\n",
                            " ('q2', 'ksoftirqd/169', '25', '863', '21583', ''),\n",
                            " ('q2', 'openssl', '870', '6923', '6023733', ''),\n",
                            " None,\n",
                            " None,\n",
                            " ('q4', 'openssl', '9809', '955', '9369210', ''),\n",
                            " ('q4', 'ksoftirqd/169', '115', '969', '111548', ''),\n",
                            " None,\n",
                            " ('q5', 'openssl', '74695', '432', '32294636', ''),\n",
                            " ('q5', 'ksoftirqd/169', '811', '441', '357987', ''),\n",
                            " None]"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 4
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "source": [
                "bpftrace_cs_pattern = r'^@(uc)\\D*(\\d+)\\D*(\\d+)\\D*(\\d+)([\\d\\D]*)'\n",
                "cs = [re.search(bpftrace_cs_pattern, item).groups()\n",
                "           if re.search(bpftrace_cs_pattern, item)\n",
                "           else None\n",
                "           for item in sample_logs]\n",
                "cs"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "[None,\n",
                            " None,\n",
                            " None,\n",
                            " None,\n",
                            " None,\n",
                            " None,\n",
                            " None,\n",
                            " None,\n",
                            " None,\n",
                            " None,\n",
                            " None,\n",
                            " None,\n",
                            " None,\n",
                            " None,\n",
                            " None]"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 5
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "from pyspark.sql.functions import regexp_extract\n",
                "\n",
                "bpftrace_cs_df = base_df.select(\n",
                "                                regexp_extract('value', bpftrace_cs_pattern, 2).alias('cs_count'),\n",
                "                                regexp_extract('value', bpftrace_cs_pattern, 3).alias('cs_average'),\n",
                "                                regexp_extract('value', bpftrace_cs_pattern, 4).alias('cs_total'),\n",
                ").filter(' cs_count != \"\"')\n",
                "\n",
                "bpftrace_cs_df.show(15)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "+--------+----------+--------+\n",
                        "|cs_count|cs_average|cs_total|\n",
                        "+--------+----------+--------+\n",
                        "|    2078|     33403|69412916|\n",
                        "|    2367|      7694|18213407|\n",
                        "|    2001|      6394|12795386|\n",
                        "|    2038|      6929|14122903|\n",
                        "|    2181|      7305|15932257|\n",
                        "|    2273|      6571|14936365|\n",
                        "|    1965|      7080|13912777|\n",
                        "|    2037|      6759|13769099|\n",
                        "|    2119|      6046|12811959|\n",
                        "|    2236|      6316|14124644|\n",
                        "|    2553|     19738|50392032|\n",
                        "|    2032|      7363|14963412|\n",
                        "|    2159|      6375|13764149|\n",
                        "|    2297|      6944|15951080|\n",
                        "|    1988|      6285|12495677|\n",
                        "+--------+----------+--------+\n",
                        "only showing top 15 rows\n",
                        "\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "source": [
                "print(bpftrace_cs_df.toPandas().to_numpy()[0])"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "['2078' '33403' '69412916']\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "source": [
                "from pyspark.sql.functions import regexp_extract\n",
                "\n",
                "bpftrace_logs_df = base_df.select(\n",
                "                                regexp_extract('value', bpftrace_ps_pattern, 1).alias('level'),\n",
                "                                regexp_extract('value', bpftrace_ps_pattern, 2).alias('program'),\n",
                "                                regexp_extract('value', bpftrace_ps_pattern, 3).alias('pkt_count'),\n",
                "                                regexp_extract('value', bpftrace_ps_pattern, 4).alias('pkt_average'),\n",
                "                                regexp_extract('value', bpftrace_ps_pattern, 5).alias('pkt_total')\n",
                "                                ).filter(' level != \"\" ')\n",
                "bpftrace_logs_df.show(15, truncate=True)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "+-----+-------------+---------+-----------+---------+\n",
                        "|level|      program|pkt_count|pkt_average|pkt_total|\n",
                        "+-----+-------------+---------+-----------+---------+\n",
                        "|   q1|      openssl|      140|       1405|   196805|\n",
                        "|   q1|ksoftirqd/169|        3|       3099|     9297|\n",
                        "|   q2|ksoftirqd/169|       25|        863|    21583|\n",
                        "|   q2|      openssl|      870|       6923|  6023733|\n",
                        "|   q4|      openssl|     9809|        955|  9369210|\n",
                        "|   q4|ksoftirqd/169|      115|        969|   111548|\n",
                        "|   q5|      openssl|    74695|        432| 32294636|\n",
                        "|   q5|ksoftirqd/169|      811|        441|   357987|\n",
                        "|   q1|      openssl|      293|       1186|   347711|\n",
                        "|   q1|ksoftirqd/169|        8|       2185|    17483|\n",
                        "|   q2|      openssl|     1564|        616|   964075|\n",
                        "|   q2|ksoftirqd/169|       43|        777|    33448|\n",
                        "|   q4|      openssl|    13323|        950| 12666103|\n",
                        "|   q4|ksoftirqd/169|       88|        989|    87061|\n",
                        "|   q5|      openssl|   101414|        430| 43703385|\n",
                        "+-----+-------------+---------+-----------+---------+\n",
                        "only showing top 15 rows\n",
                        "\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "source": [
                "from pyspark.sql.functions import regexp_extract\n",
                "\n",
                "bpftrace_cs_df = base_df.select(\n",
                "                                regexp_extract('value', bpftrace_cs_pattern, 2).alias('cs_count'),\n",
                "                                regexp_extract('value', bpftrace_cs_pattern, 3).alias('cs_average'),\n",
                "                                regexp_extract('value', bpftrace_cs_pattern, 4).alias('cs_total'),\n",
                ").filter(' cs_count != \"\"')\n",
                "\n",
                "bpftrace_cs_df.show(15)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "+--------+----------+--------+\n",
                        "|cs_count|cs_average|cs_total|\n",
                        "+--------+----------+--------+\n",
                        "|    2078|     33403|69412916|\n",
                        "|    2367|      7694|18213407|\n",
                        "|    2001|      6394|12795386|\n",
                        "|    2038|      6929|14122903|\n",
                        "|    2181|      7305|15932257|\n",
                        "|    2273|      6571|14936365|\n",
                        "|    1965|      7080|13912777|\n",
                        "|    2037|      6759|13769099|\n",
                        "|    2119|      6046|12811959|\n",
                        "|    2236|      6316|14124644|\n",
                        "|    2553|     19738|50392032|\n",
                        "|    2032|      7363|14963412|\n",
                        "|    2159|      6375|13764149|\n",
                        "|    2297|      6944|15951080|\n",
                        "|    1988|      6285|12495677|\n",
                        "+--------+----------+--------+\n",
                        "only showing top 15 rows\n",
                        "\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "source": [
                "cs_array = bpftrace_cs_df.toPandas().to_numpy()[0]\n",
                "print(cs_array)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "['2078' '33403' '69412916']\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "source": [
                "from pyspark.sql.functions import lit\n",
                "\n",
                "bpftrace_res_df = bpftrace_logs_df.withColumn(\"cs_count\", lit(cs_array[0])\n",
                "                    ).withColumn(\"cs_average\", lit(cs_array[1])\n",
                "                    ).withColumn(\"cs_total\", lit(cs_array[2]))\n",
                "bpftrace_res_df.show(15, truncate=True)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "+-----+-------------+---------+-----------+---------+--------+----------+--------+\n",
                        "|level|      program|pkt_count|pkt_average|pkt_total|cs_count|cs_average|cs_total|\n",
                        "+-----+-------------+---------+-----------+---------+--------+----------+--------+\n",
                        "|   q1|      openssl|      140|       1405|   196805|    2078|     33403|69412916|\n",
                        "|   q1|ksoftirqd/169|        3|       3099|     9297|    2078|     33403|69412916|\n",
                        "|   q2|ksoftirqd/169|       25|        863|    21583|    2078|     33403|69412916|\n",
                        "|   q2|      openssl|      870|       6923|  6023733|    2078|     33403|69412916|\n",
                        "|   q4|      openssl|     9809|        955|  9369210|    2078|     33403|69412916|\n",
                        "|   q4|ksoftirqd/169|      115|        969|   111548|    2078|     33403|69412916|\n",
                        "|   q5|      openssl|    74695|        432| 32294636|    2078|     33403|69412916|\n",
                        "|   q5|ksoftirqd/169|      811|        441|   357987|    2078|     33403|69412916|\n",
                        "|   q1|      openssl|      293|       1186|   347711|    2078|     33403|69412916|\n",
                        "|   q1|ksoftirqd/169|        8|       2185|    17483|    2078|     33403|69412916|\n",
                        "|   q2|      openssl|     1564|        616|   964075|    2078|     33403|69412916|\n",
                        "|   q2|ksoftirqd/169|       43|        777|    33448|    2078|     33403|69412916|\n",
                        "|   q4|      openssl|    13323|        950| 12666103|    2078|     33403|69412916|\n",
                        "|   q4|ksoftirqd/169|       88|        989|    87061|    2078|     33403|69412916|\n",
                        "|   q5|      openssl|   101414|        430| 43703385|    2078|     33403|69412916|\n",
                        "+-----+-------------+---------+-----------+---------+--------+----------+--------+\n",
                        "only showing top 15 rows\n",
                        "\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "source": [
                "raw_data_files = glob.glob('../ansible/results/iperf/server1-1200m-1.out')\n",
                "raw_data_files\n",
                "\n",
                "base_df = spark.read.text(raw_data_files)\n",
                "base_df.printSchema()\n",
                "\n",
                "base_df.show(10, truncate=False)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "root\n",
                        " |-- value: string (nullable = true)\n",
                        "\n",
                        "+-----+\n",
                        "|value|\n",
                        "+-----+\n",
                        "+-----+\n",
                        "\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "21/09/09 19:55:54 WARN DataSource: All paths were ignored:\n",
                        "  \n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "source": [
                "sample_logs = [item['value'] for item in base_df.take(15)]\n",
                "sample_logs\n",
                "\n",
                "iperf_s_pattern = r'[\\d\\D]*-([\\d.]*)\\D*([\\d.]*)\\D*([\\d.]*)\\D*([\\d.]*)\\D*([\\d/]*)\\D*([\\d.]*)([\\d\\D]*)'\n",
                "cs = [re.search(iperf_s_pattern, item).groups()\n",
                "           if re.search(iperf_s_pattern, item)\n",
                "           else None\n",
                "           for item in sample_logs]\n",
                "cs"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "[]"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 13
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "source": [
                "raw_data_files = glob.glob('../ansible/results/openssl/server1-1200m-1.out')\n",
                "raw_data_files\n",
                "\n",
                "base_df = spark.read.text(raw_data_files)\n",
                "base_df.printSchema()\n",
                "\n",
                "base_df.show(10, truncate=False)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "21/09/09 19:55:54 WARN DataSource: All paths were ignored:\n",
                        "  \n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "root\n",
                        " |-- value: string (nullable = true)\n",
                        "\n",
                        "+-----+\n",
                        "|value|\n",
                        "+-----+\n",
                        "+-----+\n",
                        "\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "source": [
                "sample_logs = [item['value'] for item in base_df.take(15)]\n",
                "sample_logs\n",
                "\n",
                "openssl_s_pattern = r'\\D*([\\d]*)\\D*([\\d]*)\\D*([\\d]*)\\D*([\\d]*)\\D*([\\d]*)\\D*([\\d.]*)([\\d\\D]*)'\n",
                "cs = [re.search(openssl_s_pattern, item).groups()\n",
                "           if re.search(openssl_s_pattern, item)\n",
                "           else None\n",
                "           for item in sample_logs]\n",
                "cs\n",
                "\n",
                "openssl_logs_df = base_df.select(\n",
                "                                regexp_extract('value', openssl_s_pattern, 6).alias('us_time')\n",
                "                                ).filter(' us_time != \"\" ')\n",
                "\n",
                "openssl_logs_df.show(5)\n",
                "cs_array = openssl_logs_df.toPandas().to_numpy()\n",
                "\n",
                "us_time = 0.0\n",
                "for a in cs_array:\n",
                "    us_time += float(a[0])\n",
                "\n",
                "us_time"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "+-------+\n",
                        "|us_time|\n",
                        "+-------+\n",
                        "+-------+\n",
                        "\n"
                    ]
                },
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "0.0"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 15
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "source": [
                "from pyspark.sql import DataFrame\n",
                "from pyspark.sql.types import *\n",
                "from functools import reduce\n",
                "\n",
                "# schema = StructType([])\n",
                "# results_df = sqlContext.createDataFrame(sc.emptyRDD(), schema)\n",
                "\n",
                "bpftrace_ps_pattern = r'^@(q\\d)\\[([\\d\\D]*)\\]\\D*(\\d+)\\D*(\\d+)\\D*(\\d+)([\\d\\D]*)'\n",
                "bpftrace_cs_pattern = r'^@(uc)\\D*(\\d+)\\D*(\\d+)\\D*(\\d+)([\\d\\D]*)'\n",
                "iperf_s_pattern = r'[\\d\\D]*-([\\d.]*)\\D*([\\d.]*)\\D*([\\d.]*)\\D*([\\d.]*)\\D*([\\d/]*)\\D*([\\d.]*)([\\d\\D]*)'\n",
                "openssl_s_pattern = r'\\D*([\\d]*)\\D*([\\d]*)\\D*([\\d]*)\\D*([\\d]*)\\D*([\\d]*)\\D*([\\d.]*)([\\d\\D]*)'\n",
                "\n",
                "\n",
                "def readFiletoDF(path) -> DataFrame:\n",
                "    raw_data_files = glob.glob(path)\n",
                "    raw_data_files\n",
                "    return spark.read.text(raw_data_files)\n",
                "\n",
                "def readBpftracePSLogstoDF(df: DataFrame) -> DataFrame:\n",
                "    bpftrace_logs_df = df.select(\n",
                "                                regexp_extract('value', bpftrace_ps_pattern, 1).alias('level'),\n",
                "                                regexp_extract('value', bpftrace_ps_pattern, 2).alias('program'),\n",
                "                                regexp_extract('value', bpftrace_ps_pattern, 3).alias('pkt_count'),\n",
                "                                regexp_extract('value', bpftrace_ps_pattern, 4).alias('pkt_average'),\n",
                "                                regexp_extract('value', bpftrace_ps_pattern, 5).alias('pkt_total')\n",
                "                                ).filter(' level != \"\" ')\n",
                "    return bpftrace_logs_df\n",
                "\n",
                "def concateBpftraceCSLogstoDF(df: DataFrame, ps_df: DataFrame) -> DataFrame:\n",
                "    bpftrace_cs_df = base_df.select(\n",
                "                                regexp_extract('value', bpftrace_cs_pattern, 2).alias('cs_count'),\n",
                "                                regexp_extract('value', bpftrace_cs_pattern, 3).alias('cs_average'),\n",
                "                                regexp_extract('value', bpftrace_cs_pattern, 4).alias('cs_total'),\n",
                "                                ).filter(' cs_count != \"\"')\n",
                "\n",
                "    cs_array = bpftrace_cs_df.toPandas().to_numpy()[0]\n",
                "\n",
                "    bpftrace_res_df = ps_df.withColumn(\"cs_count\", lit(cs_array[0])\n",
                "                    ).withColumn(\"cs_average\", lit(cs_array[1])\n",
                "                    ).withColumn(\"cs_total\", lit(cs_array[2]))\n",
                "    \n",
                "    return bpftrace_res_df\n",
                "\n",
                "def concateIperfLogsToDF(df: DataFrame, in_df: DataFrame) -> DataFrame:\n",
                "    iperf_logs_df = df.select(\n",
                "                                regexp_extract('value', iperf_s_pattern, 1).alias('intval'),\n",
                "                                regexp_extract('value', iperf_s_pattern, 2).alias('transfer'),\n",
                "                                regexp_extract('value', iperf_s_pattern, 3).alias('rx_bandwidth'),\n",
                "                                regexp_extract('value', iperf_s_pattern, 4).alias('jitter'),\n",
                "                                regexp_extract('value', iperf_s_pattern, 6).alias('pkt_loss')\n",
                "                                ).filter(' intval != \"\" ')\n",
                "    \n",
                "    cs_array = iperf_logs_df.toPandas().to_numpy()[0]\n",
                "\n",
                "    iperf_res_df = in_df.withColumn(\"duration\", lit(cs_array[0])\n",
                "                    ).withColumn(\"rx_gbytes\", lit(cs_array[1])\n",
                "                    ).withColumn(\"rx_bandwidth\", lit(cs_array[2])\n",
                "                    ).withColumn(\"jitter\", lit(cs_array[3])\n",
                "                    ).withColumn(\"pkt_loss\", lit(cs_array[4]))\n",
                "\n",
                "    iperf_res_df.show(5)\n",
                "    return iperf_res_df\n",
                "\n",
                "def concateOpensslLogsToDF(df: DataFrame, in_df: DataFrame) -> DataFrame:\n",
                "    openssl_logs_df = df.select(\n",
                "                                regexp_extract('value', openssl_s_pattern, 6).alias('us_time')\n",
                "                                ).filter(' us_time != \"\" ')\n",
                "    \n",
                "    cs_array = openssl_logs_df.toPandas().to_numpy()\n",
                "\n",
                "    us_time = 0.0\n",
                "    for a in cs_array:\n",
                "        us_time += float(a[0])\n",
                "\n",
                "    us_time\n",
                "\n",
                "    openssl_res_df = in_df.withColumn(\"us_time\", lit(us_time))\n",
                "\n",
                "    return openssl_res_df\n",
                "\n",
                "df_list = []\n",
                "\n",
                "for x in range(1, 3):\n",
                "    path = '../ansible/results/bpftrace/server1-{}m-{}.out'.format(1000 + 200*x, x)\n",
                "    base_df = readFiletoDF(path)\n",
                "    # base_df.show(5, truncate=False)\n",
                "    ps_df = readBpftracePSLogstoDF(base_df)\n",
                "    ps_cs_df = concateBpftraceCSLogstoDF(base_df, ps_df)\n",
                "    df_1 = ps_cs_df.withColumn(\"tx_bandwidth\", lit(1000 + 100*x))\n",
                "\n",
                "    path = '../ansible/results/iperf/server1-{}m-{}.out'.format(1000 + 200*x, x)\n",
                "    base_df = readFiletoDF(path)\n",
                "    # base_df.show(5, truncate=False)\n",
                "    df_2 = concateIperfLogsToDF(base_df, df_1)\n",
                "\n",
                "    path = '../ansible/results/openssl/server1-{}m-{}.out'.format(1000 + 200*x, x)\n",
                "    base_df = readFiletoDF(path)\n",
                "    # base_df.show(5, truncate=False)\n",
                "    df_3 = concateOpensslLogsToDF(base_df, df_2)\n",
                "\n",
                "    df_list.append(df_3)\n",
                "\n",
                "results_df = reduce(lambda x, y: x.union(y), df_list)\n",
                "# results_df.show(5, truncate=False)\n",
                "results_df.printSchema()\n"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "+-----+-------------+---------+-----------+---------+--------+----------+--------+------------+--------+---------+------------+------+--------+\n",
                        "|level|      program|pkt_count|pkt_average|pkt_total|cs_count|cs_average|cs_total|tx_bandwidth|duration|rx_gbytes|rx_bandwidth|jitter|pkt_loss|\n",
                        "+-----+-------------+---------+-----------+---------+--------+----------+--------+------------+--------+---------+------------+------+--------+\n",
                        "|   q1|      openssl|      140|       1405|   196805|    2078|     33403|69412916|        1100|    59.9|     8.37|        1.20| 0.012|    0.16|\n",
                        "|   q1|ksoftirqd/169|        3|       3099|     9297|    2078|     33403|69412916|        1100|    59.9|     8.37|        1.20| 0.012|    0.16|\n",
                        "|   q2|ksoftirqd/169|       25|        863|    21583|    2078|     33403|69412916|        1100|    59.9|     8.37|        1.20| 0.012|    0.16|\n",
                        "|   q2|      openssl|      870|       6923|  6023733|    2078|     33403|69412916|        1100|    59.9|     8.37|        1.20| 0.012|    0.16|\n",
                        "|   q4|      openssl|     9809|        955|  9369210|    2078|     33403|69412916|        1100|    59.9|     8.37|        1.20| 0.012|    0.16|\n",
                        "+-----+-------------+---------+-----------+---------+--------+----------+--------+------------+--------+---------+------------+------+--------+\n",
                        "only showing top 5 rows\n",
                        "\n",
                        "+-----+-------------+---------+-----------+---------+--------+----------+--------+------------+--------+---------+------------+------+--------+\n",
                        "|level|      program|pkt_count|pkt_average|pkt_total|cs_count|cs_average|cs_total|tx_bandwidth|duration|rx_gbytes|rx_bandwidth|jitter|pkt_loss|\n",
                        "+-----+-------------+---------+-----------+---------+--------+----------+--------+------------+--------+---------+------------+------+--------+\n",
                        "|   q1|      openssl|      159|       1624|   258300|    2058|     36771|75676154|        1200|    59.9|     9.76|        1.40| 0.009|    0.15|\n",
                        "|   q1|ksoftirqd/169|        7|       1892|    13244|    2058|     36771|75676154|        1200|    59.9|     9.76|        1.40| 0.009|    0.15|\n",
                        "|   q2|      openssl|      365|      58896| 21497099|    2058|     36771|75676154|        1200|    59.9|     9.76|        1.40| 0.009|    0.15|\n",
                        "|   q2|ksoftirqd/169|       27|     589433| 15914716|    2058|     36771|75676154|        1200|    59.9|     9.76|        1.40| 0.009|    0.15|\n",
                        "|   q4|ksoftirqd/169|      132|        924|   121993|    2058|     36771|75676154|        1200|    59.9|     9.76|        1.40| 0.009|    0.15|\n",
                        "+-----+-------------+---------+-----------+---------+--------+----------+--------+------------+--------+---------+------------+------+--------+\n",
                        "only showing top 5 rows\n",
                        "\n",
                        "root\n",
                        " |-- level: string (nullable = true)\n",
                        " |-- program: string (nullable = true)\n",
                        " |-- pkt_count: string (nullable = true)\n",
                        " |-- pkt_average: string (nullable = true)\n",
                        " |-- pkt_total: string (nullable = true)\n",
                        " |-- cs_count: string (nullable = false)\n",
                        " |-- cs_average: string (nullable = false)\n",
                        " |-- cs_total: string (nullable = false)\n",
                        " |-- tx_bandwidth: integer (nullable = false)\n",
                        " |-- duration: string (nullable = false)\n",
                        " |-- rx_gbytes: string (nullable = false)\n",
                        " |-- rx_bandwidth: string (nullable = false)\n",
                        " |-- jitter: string (nullable = false)\n",
                        " |-- pkt_loss: string (nullable = false)\n",
                        " |-- us_time: double (nullable = false)\n",
                        "\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "source": [
                "from pyspark.sql import SparkSession\n",
                "\n",
                "results_df.show(5)\n",
                "results_df.createOrReplaceTempView(\"LOGS\")\n",
                "\n",
                "\n",
                "df2=spark.sql(\"select level, sum(pkt_count) as count, avg(rx_gbytes) as rx_bytes from LOGS group by level, tx_bandwidth\")\n",
                "df2.show(15, truncate=True)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "+-----+-------------+---------+-----------+---------+--------+----------+--------+------------+--------+---------+------------+------+--------+------------------+\n",
                        "|level|      program|pkt_count|pkt_average|pkt_total|cs_count|cs_average|cs_total|tx_bandwidth|duration|rx_gbytes|rx_bandwidth|jitter|pkt_loss|           us_time|\n",
                        "+-----+-------------+---------+-----------+---------+--------+----------+--------+------------+--------+---------+------------+------+--------+------------------+\n",
                        "|   q1|      openssl|      140|       1405|   196805|    2078|     33403|69412916|        1100|    59.9|     8.37|        1.20| 0.012|    0.16|47.910000000000004|\n",
                        "|   q1|ksoftirqd/169|        3|       3099|     9297|    2078|     33403|69412916|        1100|    59.9|     8.37|        1.20| 0.012|    0.16|47.910000000000004|\n",
                        "|   q2|ksoftirqd/169|       25|        863|    21583|    2078|     33403|69412916|        1100|    59.9|     8.37|        1.20| 0.012|    0.16|47.910000000000004|\n",
                        "|   q2|      openssl|      870|       6923|  6023733|    2078|     33403|69412916|        1100|    59.9|     8.37|        1.20| 0.012|    0.16|47.910000000000004|\n",
                        "|   q4|      openssl|     9809|        955|  9369210|    2078|     33403|69412916|        1100|    59.9|     8.37|        1.20| 0.012|    0.16|47.910000000000004|\n",
                        "+-----+-------------+---------+-----------+---------+--------+----------+--------+------------+--------+---------+------------+------+--------+------------------+\n",
                        "only showing top 5 rows\n",
                        "\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": []
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "+-----+---------+-----------------+\n",
                        "|level|    count|         rx_bytes|\n",
                        "+-----+---------+-----------------+\n",
                        "|   q5|5891511.0|8.369999999999992|\n",
                        "|   q2|  38620.0|9.759999999999993|\n",
                        "|   q1|  20221.0|9.759999999999993|\n",
                        "|   q4| 891927.0|9.759999999999993|\n",
                        "|   q5|6991567.0|9.759999999999993|\n",
                        "|   q4| 787347.0|8.369999999999992|\n",
                        "|   q2|  92641.0|8.369999999999992|\n",
                        "|   q1|  17340.0|8.370000000000003|\n",
                        "+-----+---------+-----------------+\n",
                        "\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": []
                }
            ],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.8.2",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.8.2 64-bit"
        },
        "interpreter": {
            "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}