{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "msg = \"Hello World\"\n",
                "print(msg)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Hello World\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "# configure spark variables\n",
                "from pyspark.context import SparkContext\n",
                "from pyspark.sql.context import SQLContext\n",
                "from pyspark.sql.session import SparkSession\n",
                "    \n",
                "sc = SparkContext()\n",
                "sqlContext = SQLContext(sc)\n",
                "spark = SparkSession(sc)\n",
                "\n",
                "# load up other dependencies\n",
                "import re\n",
                "import pandas as pd\n",
                "\n",
                "import glob\n",
                "\n",
                "raw_data_files = glob.glob('../ansible/.results/bpftrace/server1-1200m-1.out')\n",
                "raw_data_files\n",
                "\n",
                "base_df = spark.read.text(raw_data_files)\n",
                "base_df.printSchema()\n",
                "\n",
                "base_df.show(10, truncate=False)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "21/09/09 21:56:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
                        "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
                        "Setting default log level to \"WARN\".\n",
                        "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "root\n",
                        " |-- value: string (nullable = true)\n",
                        "\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": []
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "+-------------------------------------------------------------+\n",
                        "|value                                                        |\n",
                        "+-------------------------------------------------------------+\n",
                        "|Attaching 12 probes...                                       |\n",
                        "|Tracing latency of network stack funtions. Hit Ctrl-C to end.|\n",
                        "|@q1[openssl]: count 137, average 1406, total 192689          |\n",
                        "|@q1[ksoftirqd/169]: count 6, average 2611, total 15668       |\n",
                        "|                                                             |\n",
                        "|@q2[openssl]: count 872, average 6842, total 5966375         |\n",
                        "|@q2[ksoftirqd/169]: count 31, average 903775, total 28017048 |\n",
                        "|                                                             |\n",
                        "|                                                             |\n",
                        "|@q4[ksoftirqd/169]: count 143, average 947, total 135453     |\n",
                        "+-------------------------------------------------------------+\n",
                        "only showing top 10 rows\n",
                        "\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": []
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "sample_logs = [item['value'] for item in base_df.take(15)]\n",
                "sample_logs"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "['Attaching 12 probes...',\n",
                            " 'Tracing latency of network stack funtions. Hit Ctrl-C to end.',\n",
                            " '@q1[openssl]: count 137, average 1406, total 192689',\n",
                            " '@q1[ksoftirqd/169]: count 6, average 2611, total 15668',\n",
                            " '',\n",
                            " '@q2[openssl]: count 872, average 6842, total 5966375',\n",
                            " '@q2[ksoftirqd/169]: count 31, average 903775, total 28017048',\n",
                            " '',\n",
                            " '',\n",
                            " '@q4[ksoftirqd/169]: count 143, average 947, total 135453',\n",
                            " '@q4[openssl]: count 9791, average 952, total 9323492',\n",
                            " '',\n",
                            " '@q5[openssl]: count 74553, average 434, total 32427855',\n",
                            " '@q5[ksoftirqd/169]: count 1009, average 446, total 450618',\n",
                            " '']"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 3
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "bpftrace_ps_pattern = r'^@(q\\d)\\[([\\d\\D]*)\\]\\D*(\\d+)\\D*(\\d+)\\D*(\\d+)([\\d\\D]*)'\n",
                "ps = [re.search(bpftrace_ps_pattern, item).groups()\n",
                "           if re.search(bpftrace_ps_pattern, item)\n",
                "           else None\n",
                "           for item in sample_logs]\n",
                "ps"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "[None,\n",
                            " None,\n",
                            " ('q1', 'openssl', '137', '1406', '192689', ''),\n",
                            " ('q1', 'ksoftirqd/169', '6', '2611', '15668', ''),\n",
                            " None,\n",
                            " ('q2', 'openssl', '872', '6842', '5966375', ''),\n",
                            " ('q2', 'ksoftirqd/169', '31', '903775', '28017048', ''),\n",
                            " None,\n",
                            " None,\n",
                            " ('q4', 'ksoftirqd/169', '143', '947', '135453', ''),\n",
                            " ('q4', 'openssl', '9791', '952', '9323492', ''),\n",
                            " None,\n",
                            " ('q5', 'openssl', '74553', '434', '32427855', ''),\n",
                            " ('q5', 'ksoftirqd/169', '1009', '446', '450618', ''),\n",
                            " None]"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 4
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "source": [
                "bpftrace_cs_pattern = r'^@(uc)\\D*(\\d+)\\D*(\\d+)\\D*(\\d+)([\\d\\D]*)'\n",
                "cs = [re.search(bpftrace_cs_pattern, item).groups()\n",
                "           if re.search(bpftrace_cs_pattern, item)\n",
                "           else None\n",
                "           for item in sample_logs]\n",
                "cs"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "[None,\n",
                            " None,\n",
                            " None,\n",
                            " None,\n",
                            " None,\n",
                            " None,\n",
                            " None,\n",
                            " None,\n",
                            " None,\n",
                            " None,\n",
                            " None,\n",
                            " None,\n",
                            " None,\n",
                            " None,\n",
                            " None]"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 5
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "from pyspark.sql.functions import regexp_extract\n",
                "\n",
                "bpftrace_cs_df = base_df.select(\n",
                "                                regexp_extract('value', bpftrace_cs_pattern, 2).alias('cs_count'),\n",
                "                                regexp_extract('value', bpftrace_cs_pattern, 3).alias('cs_average'),\n",
                "                                regexp_extract('value', bpftrace_cs_pattern, 4).alias('cs_total'),\n",
                ").filter(' cs_count != \"\"')\n",
                "\n",
                "bpftrace_cs_df.show(15)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "+--------+----------+--------+\n",
                        "|cs_count|cs_average|cs_total|\n",
                        "+--------+----------+--------+\n",
                        "|    2041|     32254|65831562|\n",
                        "|    2123|      6093|12935663|\n",
                        "|    2670|      6674|17821187|\n",
                        "|    2142|      6466|13852272|\n",
                        "|    2220|      6520|14475643|\n",
                        "|    2068|      6259|12944316|\n",
                        "|    2441|      6083|14850249|\n",
                        "|    2106|      5994|12624615|\n",
                        "|    2081|      5402|11241927|\n",
                        "|    2175|      6186|13455724|\n",
                        "|    2993|     12533|37513451|\n",
                        "|    2248|     10055|22603743|\n",
                        "|    2168|      7632|16546510|\n",
                        "|    2065|      5878|12138880|\n",
                        "|    2597|      6218|16149213|\n",
                        "+--------+----------+--------+\n",
                        "only showing top 15 rows\n",
                        "\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "source": [
                "print(bpftrace_cs_df.toPandas().to_numpy()[0])"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "['2041' '32254' '65831562']\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "source": [
                "from pyspark.sql.functions import regexp_extract\n",
                "\n",
                "bpftrace_logs_df = base_df.select(\n",
                "                                regexp_extract('value', bpftrace_ps_pattern, 1).alias('level'),\n",
                "                                regexp_extract('value', bpftrace_ps_pattern, 2).alias('program'),\n",
                "                                regexp_extract('value', bpftrace_ps_pattern, 3).alias('pkt_count'),\n",
                "                                regexp_extract('value', bpftrace_ps_pattern, 4).alias('pkt_average'),\n",
                "                                regexp_extract('value', bpftrace_ps_pattern, 5).alias('pkt_total')\n",
                "                                ).filter(' level != \"\" ')\n",
                "bpftrace_logs_df.show(15, truncate=True)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "+-----+-------------+---------+-----------+---------+\n",
                        "|level|      program|pkt_count|pkt_average|pkt_total|\n",
                        "+-----+-------------+---------+-----------+---------+\n",
                        "|   q1|      openssl|      137|       1406|   192689|\n",
                        "|   q1|ksoftirqd/169|        6|       2611|    15668|\n",
                        "|   q2|      openssl|      872|       6842|  5966375|\n",
                        "|   q2|ksoftirqd/169|       31|     903775| 28017048|\n",
                        "|   q4|ksoftirqd/169|      143|        947|   135453|\n",
                        "|   q4|      openssl|     9791|        952|  9323492|\n",
                        "|   q5|      openssl|    74553|        434| 32427855|\n",
                        "|   q5|ksoftirqd/169|     1009|        446|   450618|\n",
                        "|   q1|      openssl|      425|       1190|   505868|\n",
                        "|   q1|ksoftirqd/169|       13|       1686|    21920|\n",
                        "|   q1|kworker/169:2|        3|       3867|    11602|\n",
                        "|   q2|      openssl|     1702|        611|  1040884|\n",
                        "|   q2|ksoftirqd/169|       48|        754|    36236|\n",
                        "|   q2|kworker/169:2|        3|       3035|     9107|\n",
                        "|   q4|      openssl|    13435|        946| 12711296|\n",
                        "+-----+-------------+---------+-----------+---------+\n",
                        "only showing top 15 rows\n",
                        "\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "source": [
                "from pyspark.sql.functions import regexp_extract\n",
                "\n",
                "bpftrace_cs_df = base_df.select(\n",
                "                                regexp_extract('value', bpftrace_cs_pattern, 2).alias('cs_count'),\n",
                "                                regexp_extract('value', bpftrace_cs_pattern, 3).alias('cs_average'),\n",
                "                                regexp_extract('value', bpftrace_cs_pattern, 4).alias('cs_total'),\n",
                ").filter(' cs_count != \"\"')\n",
                "\n",
                "bpftrace_cs_df.show(15)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "+--------+----------+--------+\n",
                        "|cs_count|cs_average|cs_total|\n",
                        "+--------+----------+--------+\n",
                        "|    2041|     32254|65831562|\n",
                        "|    2123|      6093|12935663|\n",
                        "|    2670|      6674|17821187|\n",
                        "|    2142|      6466|13852272|\n",
                        "|    2220|      6520|14475643|\n",
                        "|    2068|      6259|12944316|\n",
                        "|    2441|      6083|14850249|\n",
                        "|    2106|      5994|12624615|\n",
                        "|    2081|      5402|11241927|\n",
                        "|    2175|      6186|13455724|\n",
                        "|    2993|     12533|37513451|\n",
                        "|    2248|     10055|22603743|\n",
                        "|    2168|      7632|16546510|\n",
                        "|    2065|      5878|12138880|\n",
                        "|    2597|      6218|16149213|\n",
                        "+--------+----------+--------+\n",
                        "only showing top 15 rows\n",
                        "\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "source": [
                "cs_array = bpftrace_cs_df.toPandas().to_numpy()[0]\n",
                "print(cs_array)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "['2041' '32254' '65831562']\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "source": [
                "from pyspark.sql.functions import lit\n",
                "\n",
                "bpftrace_res_df = bpftrace_logs_df.withColumn(\"cs_count\", lit(cs_array[0])\n",
                "                    ).withColumn(\"cs_average\", lit(cs_array[1])\n",
                "                    ).withColumn(\"cs_total\", lit(cs_array[2]))\n",
                "bpftrace_res_df.show(15, truncate=True)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "+-----+-------------+---------+-----------+---------+--------+----------+--------+\n",
                        "|level|      program|pkt_count|pkt_average|pkt_total|cs_count|cs_average|cs_total|\n",
                        "+-----+-------------+---------+-----------+---------+--------+----------+--------+\n",
                        "|   q1|      openssl|      137|       1406|   192689|    2041|     32254|65831562|\n",
                        "|   q1|ksoftirqd/169|        6|       2611|    15668|    2041|     32254|65831562|\n",
                        "|   q2|      openssl|      872|       6842|  5966375|    2041|     32254|65831562|\n",
                        "|   q2|ksoftirqd/169|       31|     903775| 28017048|    2041|     32254|65831562|\n",
                        "|   q4|ksoftirqd/169|      143|        947|   135453|    2041|     32254|65831562|\n",
                        "|   q4|      openssl|     9791|        952|  9323492|    2041|     32254|65831562|\n",
                        "|   q5|      openssl|    74553|        434| 32427855|    2041|     32254|65831562|\n",
                        "|   q5|ksoftirqd/169|     1009|        446|   450618|    2041|     32254|65831562|\n",
                        "|   q1|      openssl|      425|       1190|   505868|    2041|     32254|65831562|\n",
                        "|   q1|ksoftirqd/169|       13|       1686|    21920|    2041|     32254|65831562|\n",
                        "|   q1|kworker/169:2|        3|       3867|    11602|    2041|     32254|65831562|\n",
                        "|   q2|      openssl|     1702|        611|  1040884|    2041|     32254|65831562|\n",
                        "|   q2|ksoftirqd/169|       48|        754|    36236|    2041|     32254|65831562|\n",
                        "|   q2|kworker/169:2|        3|       3035|     9107|    2041|     32254|65831562|\n",
                        "|   q4|      openssl|    13435|        946| 12711296|    2041|     32254|65831562|\n",
                        "+-----+-------------+---------+-----------+---------+--------+----------+--------+\n",
                        "only showing top 15 rows\n",
                        "\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "source": [
                "raw_data_files = glob.glob('../ansible/.results/iperf/server1-1200m-1.out')\n",
                "raw_data_files\n",
                "\n",
                "base_df = spark.read.text(raw_data_files)\n",
                "base_df.printSchema()\n",
                "\n",
                "base_df.show(10, truncate=False)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "root\n",
                        " |-- value: string (nullable = true)\n",
                        "\n",
                        "+---------------------------------------------------------------------------------+\n",
                        "|value                                                                            |\n",
                        "+---------------------------------------------------------------------------------+\n",
                        "|------------------------------------------------------------                     |\n",
                        "|Server listening on UDP port 5001                                                |\n",
                        "|Receiving 1470 byte datagrams                                                    |\n",
                        "|UDP buffer size:  208 KByte (default)                                            |\n",
                        "|------------------------------------------------------------                     |\n",
                        "|[  3] local 10.0.0.2 port 5001 connected with 10.0.0.1 port 39183                |\n",
                        "|[ ID] Interval       Transfer     Bandwidth        Jitter   Lost/Total Datagrams |\n",
                        "|[  3]  0.0-60.0 sec  8.38 GBytes  1.20 Gbits/sec   0.012 ms 2800/6122450 (0.046%)|\n",
                        "+---------------------------------------------------------------------------------+\n",
                        "\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "source": [
                "sample_logs = [item['value'] for item in base_df.take(15)]\n",
                "sample_logs\n",
                "\n",
                "iperf_s_pattern = r'[\\d\\D]*-([\\d.]*)\\D*([\\d.]*)\\D*([\\d.]*)\\D*([\\d.]*)\\D*([\\d/]*)\\D*([\\d.]*)([\\d\\D]*)'\n",
                "cs = [re.search(iperf_s_pattern, item).groups()\n",
                "           if re.search(iperf_s_pattern, item)\n",
                "           else None\n",
                "           for item in sample_logs]\n",
                "cs"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "[('', '', '', '', '', '', ''),\n",
                            " None,\n",
                            " None,\n",
                            " None,\n",
                            " ('', '', '', '', '', '', ''),\n",
                            " None,\n",
                            " None,\n",
                            " ('60.0', '8.38', '1.20', '0.012', '2800/6122450', '0.046', '%)')]"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 13
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "source": [
                "raw_data_files = glob.glob('../ansible/.results/openssl/server1-1200m-1.out')\n",
                "raw_data_files\n",
                "\n",
                "base_df = spark.read.text(raw_data_files)\n",
                "base_df.printSchema()\n",
                "\n",
                "base_df.show(10, truncate=False)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "root\n",
                        " |-- value: string (nullable = true)\n",
                        "\n",
                        "+-----------------------------------------------------------------------------+\n",
                        "|value                                                                        |\n",
                        "+-----------------------------------------------------------------------------+\n",
                        "|Doing aes-256 cbc for 10s on 16 size blocks: 88870772 aes-256 cbc's in 8.34s |\n",
                        "|Doing aes-256 cbc for 10s on 64 size blocks: 21869876 aes-256 cbc's in 7.63s |\n",
                        "|Doing aes-256 cbc for 10s on 256 size blocks: 5490295 aes-256 cbc's in 7.66s |\n",
                        "|Doing aes-256 cbc for 10s on 1024 size blocks: 1379086 aes-256 cbc's in 7.62s|\n",
                        "|Doing aes-256 cbc for 10s on 8192 size blocks: 171294 aes-256 cbc's in 7.59s |\n",
                        "|Doing aes-256 cbc for 10s on 16384 size blocks: 85618 aes-256 cbc's in 7.56s |\n",
                        "+-----------------------------------------------------------------------------+\n",
                        "\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "source": [
                "sample_logs = [item['value'] for item in base_df.take(15)]\n",
                "sample_logs\n",
                "\n",
                "openssl_s_pattern = r'\\D*([\\d]*)\\D*([\\d]*)\\D*([\\d]*)\\D*([\\d]*)\\D*([\\d]*)\\D*([\\d.]*)([\\d\\D]*)'\n",
                "cs = [re.search(openssl_s_pattern, item).groups()\n",
                "           if re.search(openssl_s_pattern, item)\n",
                "           else None\n",
                "           for item in sample_logs]\n",
                "cs\n",
                "\n",
                "openssl_logs_df = base_df.select(\n",
                "                                regexp_extract('value', openssl_s_pattern, 6).alias('us_time')\n",
                "                                ).filter(' us_time != \"\" ')\n",
                "\n",
                "openssl_logs_df.show(5)\n",
                "cs_array = openssl_logs_df.toPandas().to_numpy()\n",
                "\n",
                "us_time = 0.0\n",
                "for a in cs_array:\n",
                "    us_time += float(a[0])\n",
                "\n",
                "us_time"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "+-------+\n",
                        "|us_time|\n",
                        "+-------+\n",
                        "|   8.34|\n",
                        "|   7.63|\n",
                        "|   7.66|\n",
                        "|   7.62|\n",
                        "|   7.59|\n",
                        "+-------+\n",
                        "only showing top 5 rows\n",
                        "\n"
                    ]
                },
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "46.400000000000006"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 15
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "source": [
                "from pyspark.sql import DataFrame\n",
                "from pyspark.sql.types import *\n",
                "from functools import reduce\n",
                "\n",
                "# schema = StructType([])\n",
                "# results_df = sqlContext.createDataFrame(sc.emptyRDD(), schema)\n",
                "\n",
                "bpftrace_ps_pattern = r'^@(q\\d)\\[([\\d\\D]*)\\]\\D*(\\d+)\\D*(\\d+)\\D*(\\d+)([\\d\\D]*)'\n",
                "bpftrace_cs_pattern = r'^@(uc)\\D*(\\d+)\\D*(\\d+)\\D*(\\d+)([\\d\\D]*)'\n",
                "iperf_s_pattern = r'[\\d\\D]*-([\\d.]*)\\D*([\\d.]*)\\D*([\\d.]*)\\D*([\\d.]*)\\D*([\\d/]*)\\D*([\\d.]*)([\\d\\D]*)'\n",
                "openssl_s_pattern = r'\\D*([\\d]*)\\D*([\\d]*)\\D*([\\d]*)\\D*([\\d]*)\\D*([\\d]*)\\D*([\\d.]*)([\\d\\D]*)'\n",
                "\n",
                "\n",
                "def readFiletoDF(path) -> DataFrame:\n",
                "    raw_data_files = glob.glob(path)\n",
                "    raw_data_files\n",
                "    return spark.read.text(raw_data_files)\n",
                "\n",
                "def readBpftracePSLogstoDF(df: DataFrame) -> DataFrame:\n",
                "    bpftrace_logs_df = df.select(\n",
                "                                regexp_extract('value', bpftrace_ps_pattern, 1).alias('level'),\n",
                "                                regexp_extract('value', bpftrace_ps_pattern, 2).alias('program'),\n",
                "                                regexp_extract('value', bpftrace_ps_pattern, 3).alias('pkt_count'),\n",
                "                                regexp_extract('value', bpftrace_ps_pattern, 4).alias('pkt_average'),\n",
                "                                regexp_extract('value', bpftrace_ps_pattern, 5).alias('pkt_total')\n",
                "                                ).filter(' level != \"\" ')\n",
                "    return bpftrace_logs_df\n",
                "\n",
                "def concateBpftraceCSLogstoDF(df: DataFrame, ps_df: DataFrame) -> DataFrame:\n",
                "    bpftrace_cs_df = base_df.select(\n",
                "                                regexp_extract('value', bpftrace_cs_pattern, 2).alias('cs_count'),\n",
                "                                regexp_extract('value', bpftrace_cs_pattern, 3).alias('cs_average'),\n",
                "                                regexp_extract('value', bpftrace_cs_pattern, 4).alias('cs_total'),\n",
                "                                ).filter(' cs_count != \"\"')\n",
                "\n",
                "    cs_array = bpftrace_cs_df.toPandas().to_numpy()[0]\n",
                "\n",
                "    bpftrace_res_df = ps_df.withColumn(\"cs_count\", lit(cs_array[0])\n",
                "                    ).withColumn(\"cs_average\", lit(cs_array[1])\n",
                "                    ).withColumn(\"cs_total\", lit(cs_array[2]))\n",
                "    \n",
                "    return bpftrace_res_df\n",
                "\n",
                "def concateIperfLogsToDF(df: DataFrame, in_df: DataFrame) -> DataFrame:\n",
                "    iperf_logs_df = df.select(\n",
                "                                regexp_extract('value', iperf_s_pattern, 1).alias('intval'),\n",
                "                                regexp_extract('value', iperf_s_pattern, 2).alias('transfer'),\n",
                "                                regexp_extract('value', iperf_s_pattern, 3).alias('rx_bandwidth'),\n",
                "                                regexp_extract('value', iperf_s_pattern, 4).alias('jitter'),\n",
                "                                regexp_extract('value', iperf_s_pattern, 6).alias('pkt_loss')\n",
                "                                ).filter(' intval != \"\" ')\n",
                "    \n",
                "    cs_array = iperf_logs_df.toPandas().to_numpy()[0]\n",
                "\n",
                "    iperf_res_df = in_df.withColumn(\"duration\", lit(cs_array[0])\n",
                "                    ).withColumn(\"rx_gbytes\", lit(cs_array[1])\n",
                "                    ).withColumn(\"rx_bandwidth\", lit(cs_array[2])\n",
                "                    ).withColumn(\"jitter\", lit(cs_array[3])\n",
                "                    ).withColumn(\"pkt_loss\", lit(cs_array[4]))\n",
                "\n",
                "    iperf_res_df.show(5)\n",
                "    return iperf_res_df\n",
                "\n",
                "def concateOpensslLogsToDF(df: DataFrame, in_df: DataFrame) -> DataFrame:\n",
                "    openssl_logs_df = df.select(\n",
                "                                regexp_extract('value', openssl_s_pattern, 6).alias('us_time')\n",
                "                                ).filter(' us_time != \"\" ')\n",
                "    \n",
                "    cs_array = openssl_logs_df.toPandas().to_numpy()\n",
                "\n",
                "    us_time = 0.0\n",
                "    for a in cs_array:\n",
                "        us_time += float(a[0])\n",
                "\n",
                "    us_time\n",
                "\n",
                "    openssl_res_df = in_df.withColumn(\"us_time\", lit(us_time))\n",
                "\n",
                "    return openssl_res_df\n",
                "\n",
                "df_list = []\n",
                "\n",
                "for x in range(1, 3):\n",
                "    path = '../ansible/.results/bpftrace/server1-{}m-{}.out'.format(1000 + 200*x, x)\n",
                "    base_df = readFiletoDF(path)\n",
                "    # base_df.show(5, truncate=False)\n",
                "    ps_df = readBpftracePSLogstoDF(base_df)\n",
                "    ps_cs_df = concateBpftraceCSLogstoDF(base_df, ps_df)\n",
                "    df_1 = ps_cs_df.withColumn(\"tx_bandwidth\", lit(1000 + 200*x))\n",
                "\n",
                "    path = '../ansible/.results/iperf/server1-{}m-{}.out'.format(1000 + 200*x, x)\n",
                "    base_df = readFiletoDF(path)\n",
                "    # base_df.show(5, truncate=False)\n",
                "    df_2 = concateIperfLogsToDF(base_df, df_1)\n",
                "\n",
                "    path = '../ansible/.results/openssl/server1-{}m-{}.out'.format(1000 + 200*x, x)\n",
                "    base_df = readFiletoDF(path)\n",
                "    # base_df.show(5, truncate=False)\n",
                "    df_3 = concateOpensslLogsToDF(base_df, df_2)\n",
                "\n",
                "    df_list.append(df_3)\n",
                "\n",
                "results_df = reduce(lambda x, y: x.union(y), df_list)\n",
                "# results_df.show(5, truncate=False)\n",
                "results_df.printSchema()\n"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "+-----+-------------+---------+-----------+---------+--------+----------+--------+------------+--------+---------+------------+------+--------+\n",
                        "|level|      program|pkt_count|pkt_average|pkt_total|cs_count|cs_average|cs_total|tx_bandwidth|duration|rx_gbytes|rx_bandwidth|jitter|pkt_loss|\n",
                        "+-----+-------------+---------+-----------+---------+--------+----------+--------+------------+--------+---------+------------+------+--------+\n",
                        "|   q1|      openssl|      137|       1406|   192689|    2041|     32254|65831562|        1200|    60.0|     8.38|        1.20| 0.012|   0.046|\n",
                        "|   q1|ksoftirqd/169|        6|       2611|    15668|    2041|     32254|65831562|        1200|    60.0|     8.38|        1.20| 0.012|   0.046|\n",
                        "|   q2|      openssl|      872|       6842|  5966375|    2041|     32254|65831562|        1200|    60.0|     8.38|        1.20| 0.012|   0.046|\n",
                        "|   q2|ksoftirqd/169|       31|     903775| 28017048|    2041|     32254|65831562|        1200|    60.0|     8.38|        1.20| 0.012|   0.046|\n",
                        "|   q4|ksoftirqd/169|      143|        947|   135453|    2041|     32254|65831562|        1200|    60.0|     8.38|        1.20| 0.012|   0.046|\n",
                        "+-----+-------------+---------+-----------+---------+--------+----------+--------+------------+--------+---------+------------+------+--------+\n",
                        "only showing top 5 rows\n",
                        "\n",
                        "+-----+-------------+---------+-----------+---------+--------+----------+--------+------------+--------+---------+------------+------+--------+\n",
                        "|level|      program|pkt_count|pkt_average|pkt_total|cs_count|cs_average|cs_total|tx_bandwidth|duration|rx_gbytes|rx_bandwidth|jitter|pkt_loss|\n",
                        "+-----+-------------+---------+-----------+---------+--------+----------+--------+------------+--------+---------+------------+------+--------+\n",
                        "|   q1|      openssl|      166|       1432|   237810|    2092|     36623|76616227|        1400|    59.9|     9.76|        1.40| 0.008|    0.16|\n",
                        "|   q1|ksoftirqd/169|        3|       4839|    14517|    2092|     36623|76616227|        1400|    59.9|     9.76|        1.40| 0.008|    0.16|\n",
                        "|   q2|ksoftirqd/169|       18|        873|    15717|    2092|     36623|76616227|        1400|    59.9|     9.76|        1.40| 0.008|    0.16|\n",
                        "|   q2|      openssl|      375|      48760| 18285302|    2092|     36623|76616227|        1400|    59.9|     9.76|        1.40| 0.008|    0.16|\n",
                        "|   q4|ksoftirqd/169|      120|        954|   114497|    2092|     36623|76616227|        1400|    59.9|     9.76|        1.40| 0.008|    0.16|\n",
                        "+-----+-------------+---------+-----------+---------+--------+----------+--------+------------+--------+---------+------------+------+--------+\n",
                        "only showing top 5 rows\n",
                        "\n",
                        "root\n",
                        " |-- level: string (nullable = true)\n",
                        " |-- program: string (nullable = true)\n",
                        " |-- pkt_count: string (nullable = true)\n",
                        " |-- pkt_average: string (nullable = true)\n",
                        " |-- pkt_total: string (nullable = true)\n",
                        " |-- cs_count: string (nullable = false)\n",
                        " |-- cs_average: string (nullable = false)\n",
                        " |-- cs_total: string (nullable = false)\n",
                        " |-- tx_bandwidth: integer (nullable = false)\n",
                        " |-- duration: string (nullable = false)\n",
                        " |-- rx_gbytes: string (nullable = false)\n",
                        " |-- rx_bandwidth: string (nullable = false)\n",
                        " |-- jitter: string (nullable = false)\n",
                        " |-- pkt_loss: string (nullable = false)\n",
                        " |-- us_time: double (nullable = false)\n",
                        "\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "source": [
                "from pyspark.sql import SparkSession\n",
                "\n",
                "results_df.show(5)\n",
                "results_df.createOrReplaceTempView(\"LOGS\")\n",
                "\n",
                "\n",
                "df2=spark.sql(\"select level, tx_bandwidth, sum(pkt_count) as count, avg(rx_gbytes) as rx_bytes from LOGS group by level, tx_bandwidth\")\n",
                "df2.show(15, truncate=True)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "+-----+-------------+---------+-----------+---------+--------+----------+--------+------------+--------+---------+------------+------+--------+------------------+\n",
                        "|level|      program|pkt_count|pkt_average|pkt_total|cs_count|cs_average|cs_total|tx_bandwidth|duration|rx_gbytes|rx_bandwidth|jitter|pkt_loss|           us_time|\n",
                        "+-----+-------------+---------+-----------+---------+--------+----------+--------+------------+--------+---------+------------+------+--------+------------------+\n",
                        "|   q1|      openssl|      137|       1406|   192689|    2041|     32254|65831562|        1200|    60.0|     8.38|        1.20| 0.012|   0.046|46.400000000000006|\n",
                        "|   q1|ksoftirqd/169|        6|       2611|    15668|    2041|     32254|65831562|        1200|    60.0|     8.38|        1.20| 0.012|   0.046|46.400000000000006|\n",
                        "|   q2|      openssl|      872|       6842|  5966375|    2041|     32254|65831562|        1200|    60.0|     8.38|        1.20| 0.012|   0.046|46.400000000000006|\n",
                        "|   q2|ksoftirqd/169|       31|     903775| 28017048|    2041|     32254|65831562|        1200|    60.0|     8.38|        1.20| 0.012|   0.046|46.400000000000006|\n",
                        "|   q4|ksoftirqd/169|      143|        947|   135453|    2041|     32254|65831562|        1200|    60.0|     8.38|        1.20| 0.012|   0.046|46.400000000000006|\n",
                        "+-----+-------------+---------+-----------+---------+--------+----------+--------+------------+--------+---------+------------+------+--------+------------------+\n",
                        "only showing top 5 rows\n",
                        "\n",
                        "+-----+------------+---------+-----------------+\n",
                        "|level|tx_bandwidth|    count|         rx_bytes|\n",
                        "+-----+------------+---------+-----------------+\n",
                        "|   q1|        1400|  20427.0|9.759999999999993|\n",
                        "|   q2|        1400|  36136.0|9.759999999999993|\n",
                        "|   q2|        1200| 117385.0|8.380000000000031|\n",
                        "|   q1|        1200|  42331.0| 8.38000000000003|\n",
                        "|   q4|        1200| 795848.0|8.380000000000031|\n",
                        "|   q5|        1200|5891180.0|8.380000000000031|\n",
                        "|   q5|        1400|6873607.0|9.759999999999993|\n",
                        "|   q4|        1400| 889737.0|9.759999999999993|\n",
                        "+-----+------------+---------+-----------------+\n",
                        "\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "%matplotlib inline\n",
                "import matplotlib.pyplot as plt\n",
                "import pandas as pd\n",
                "\n",
                "fig = plt.figure(figsize=(8,6), dpi=80)\n",
                "\n",
                "ax = fig.add_axes([0,0,1,1])\n",
                "\n",
                "df3=spark.sql(\"select level, tx_bandwidth, sum(pkt_total) as totalfrom LOGS group by level, tx_bandwidth\")\n",
                "\n",
                "df3.plot(kind='bar')"
            ],
            "outputs": [
                {
                    "output_type": "error",
                    "ename": "ModuleNotFoundError",
                    "evalue": "No module named 'sklearn'",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
                        "\u001b[0;32m/var/folders/p4/vbh79z3d78xb8b1frmchv0wm0000gn/T/ipykernel_30039/2770871116.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
                    ]
                }
            ],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.8.2",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.8.2 64-bit"
        },
        "interpreter": {
            "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}